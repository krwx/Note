# 多进程

- [多进程](#多进程)
  - [Process](#process)
    - [超时控制](#超时控制)
  - [上下文和启动方法](#上下文和启动方法)
    - [不同操作系统使用的上下文](#不同操作系统使用的上下文)
    - [指定上下文](#指定上下文)
  - [Pool](#pool)
    - [参考](#参考)
    - [使用](#使用)
    - [AsyncResult](#asyncresult)
  - [进程间通信](#进程间通信)
    - [Queue](#queue)
    - [Pipe](#pipe)
  - [进程间同步](#进程间同步)
  - [进程间共享状态](#进程间共享状态)
    - [共享内存](#共享内存)
    - [服务进程（Manager）](#服务进程manager)
  - [multiprocessing.Lock() 与 Manager().Lock() 的区别](#multiprocessinglock-与-managerlock-的区别)
  - [参考](#参考-1)
    - [Process 类](#process-类)

## Process

使用 Process 类创建一个进程：

```py
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
```

执行结果：

```txt
Parent process 928.
Child process will start.
Run child process test (929)...
Process end.
```

创建子进程时，**只需要传入一个执行函数和函数的参数（注意为 `args=('test',)` 这种格式）**，创建一个 `Process` 实例，用 `start()` 方法启动，用 `join()` 方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。

### 超时控制

使用 `.join(timeout)` 等待 timeout 秒后，通过 `.is_alive()` 判断进程是否存活，如果还存活则调用 `.terminate()` 终结进程

超时 5 秒则中断进程：

```py
from multiprocessing import get_context

const TIMEOUT_SECONDS = 5

def timeout_control():
    ctx = get_context("spawn")
    result_queue = ctx.Queue()
    process = ctx.Process(
        target=test_func,
        args=(task_name,),
        name=f"test_process",
    )
    process.start()
    try:
        process.join(TIMEOUT_SECONDS)
        if process.is_alive():
            process.terminate()
            process.join()
            raise CustomTimeoutError(
                f"task exceeded {TIMEOUT_SECONDS} seconds"
            )
```

## 上下文和启动方法

multiprocessing 支持三种启动进程的方法。这些 启动方法 有

- `spawn`
  - 父进程会启动一个新的 Python 解释器进程。 子进程将只继承那些运行进程对象的 run() 方法所必须的资源。 特别地，来自父进程的非必需文件描述符和句柄将不会被继承。 使用此方法启动进程相比使用 fork 或 forkserver 要慢上许多。
  - 在 POSIX 和 `Windows` 平台上可用。 默认在 Windows 和 macOS 上。
- `fork`
  - 父进程使用 `os.fork()` 来产生 Python 解释器分叉。子进程在开始时实际上与父进程相同。父进程的所有资源都由子进程继承。请注意，安全分叉多线程进程是棘手的。
  - 主要在 Unix/Linux/Mac 系统使用

有多种启动进程方法的原因是适配不同的平台

### 不同操作系统使用的上下文

在 Python 的 `multiprocessing` 模块中，**如果没有指定上下文，系统会根据操作系统选择默认的上下文**：

1. Unix/Linux/macOS 系统

    - **默认上下文**：`fork`
    - 使用 `fork` 方法创建子进程，会继承父进程的所有资源
    - 在 macOS 10.13+ 和 Python 3.8+ 中，由于安全考虑，默认改为 `spawn`

2. Windows 系统

    - **默认上下文**：`spawn`
    - 从头开始启动新的 Python 解释器进程

3. macOS（特定版本）

    - 从 Python 3.8 开始，macOS 默认使用 `spawn`
    - 因为 `fork` 在 macOS 上可能导致与系统框架的兼容性问题

### 指定上下文

1、`set_start_method()`

要选择一个启动方法，你应该在主模块的 `if __name__ == '__main__'` 子句中调用 `set_start_method()` 。例如：

```py
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    mp.set_start_method('spawn')
    q = mp.Queue()
    p = mp.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```

> 在程序中 `set_start_method()` 不应该被多次调用。

或者，你可以使用 `get_context()` 来获取上下文对象。上下文对象与 multiprocessing 模块具有相同的API，并允许在同一程序中使用多种启动方法。:

```py
import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    ctx = mp.get_context('spawn')
    q = ctx.Queue()
    p = ctx.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```

> 请注意，对象在不同上下文创建的进程间可能并不兼容。 特别是，使用 `fork` 上下文创建的锁不能传递给使用 `spawn` 或 `forkserver` 启动方法启动的进程。

查看当前上下文：

```python
import multiprocessing

# 1. 查看当前平台的默认启动方法
print("默认启动方法:", multiprocessing.get_start_method())
# 可能的输出: 'fork' 或 'spawn'

# 2. 使用默认上下文（不指定时的行为）
ctx = multiprocessing.get_context()  # 获取默认上下文
# 等价于不指定上下文
```

## Pool

### 参考

语法：`Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])¶`

- `processes` 是要使用的工作进程数目。如果 `processes` 为 `None`，则使用 `os.cpu_count()` 返回的值。(3.13 使用 `os.process_cpu_count()`)

`Pool` 的方法：

- `apply(func[, args[, kwds]])`
  - **同步执行，会阻塞**
  - 使用 `args` 参数以及 `kwds` 命名参数调用 `func` , 它会**返回结果前阻塞**。
- `apply_async(func[, args[, kwds[, callback[, error_callback]]]])`
  - **异步执行，不会阻塞**，更适合并行化工作
  - `apply()` 方法的一个变种，返回一个 `AsyncResult` 对象。
  - 如果指定了 `callback` , 它必须是一个**接受单个参数的可调用对象**。
  - 当执行成功时，`callback` 会被用于处理执行后的返回结果，否则，调用 `error_callback` 。
  - 回调函数应该立即执行完成，否则会阻塞负责处理结果的线程。
- `close()`
  - 阻止后续任务提交到进程池，当所有任务执行完成后，工作进程会退出。
- `terminate()`
  - 不必等待未完成的任务，立即停止工作进程。当进程池对象被垃圾回收时，会立即调用 `terminate()`。
- `join()`
  - 等待工作进程结束。调用 `join()` 前必须先调用 `close()` 或者 `terminate()` 。

### 使用

如果要启动大量的子进程，可以用进程池的方式批量创建子进程：

```py
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Pool(4) # 最多跑 4 个进程
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
```

执行结果如下：

```txt
Parent process 669.
Waiting for all subprocesses done...
Run task 0 (671)...
Run task 1 (672)...
Run task 2 (673)...
Run task 3 (674)...
Task 2 runs 0.14 seconds.
Run task 4 (673)...
Task 1 runs 0.27 seconds.
Task 3 runs 0.86 seconds.
Task 0 runs 1.41 seconds.
Task 4 runs 1.91 seconds.
All subprocesses done.
```

对 `Pool` 对象调用 `join()` 方法会**等待所有子进程执行完毕**，调用 `join()` 之前必须先调用 `close()` ，调用 `close()` 之后就不能继续添加新的 `Process`了。

请注意输出的结果，`task 0，1，2，3` 是立刻执行的，而 `task 4` 要等待前面某个 `task` 完成后才执行

### AsyncResult

`Pool.apply_async()` 和 `Pool.map_async()` 返回对象所属的类。

- `get([timeout])`
  - 用于获取执行结果。
  - 如果 `timeout` 不是 `None` 并且在 `timeout` 秒内仍然没有执行完得到结果，则抛出 `multiprocessing.TimeoutError` 异常。
- `wait([timeout])`
  - 阻塞，直到返回结果，或者 `timeout` 秒后超时。
- `ready()`
  - 返回执行状态，是否已经完成。
- `successful()`
  - 判断调用是否已经完成并且未引发异常。 如果还未获得结果则将引发 `ValueError`。

在 `pool.join()` 前调用 `get()`，会造成阻塞：

```py
from multiprocessing import Pool
import time, random


def f(x):
    time.sleep(random.random() * 3)
    return x * x


def main():
    pool = Pool(processes=4)
    t_start = time.time()
    for i in range(5):
        result = pool.apply_async(f, (i,))
        print(result.get())
    pool.close()
    pool.join()
    print(time.time() - t_start)

""" 
输出：
0
1
4
9
16
7.997464895248413
"""
```

在 `pool.join()` 后调用 `get()`，不会造成阻塞：

```py
def f(x):
    time.sleep(random.random() * 3)
    return x * x


def main():
    pool = Pool(processes=4)
    t_start = time.time()
    res_list = []
    for i in range(5):
        result = pool.apply_async(f, (i,))
        res_list.append(result)
    pool.close()
    pool.join()
    for res in res_list:
        print(res.get())
    print(time.time() - t_start)

""" 
输出：
9
0
16
1
4
2.55433548671
"""
```

## 进程间通信

Python 的 `multiprocessing` 模块包装了底层的机制，提供了 `Queue、Pipes` 多种方式来交换数据。

进程之间发送数据

### Queue

我们以 `Queue` 为例，在父进程中创建两个子进程，一个往 `Queue` 里写数据，一个从 `Queue` 里读数据：

```py
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
```

运行结果如下：

```txt
Process to write: 50563
Put A to queue...
Process to read: 50564
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.
```

### Pipe

`Pipe()` 函数返回一个由管道连接的连接对象，默认情况下是双工（双向）。例如:

```py
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # 打印 "[42, None, 'hello']"
    p.join()
```

返回的两个连接对象 `Pipe()` 表示管道的两端。每个连接对象都有 `send()` 和 `recv()` 方法（相互之间的）。请注意，如果两个进程（或线程）同时尝试读取或写入管道的 同一 端，则管道中的数据可能会损坏。

## 进程间同步

`multiprocessing` 包含来自 `threading` 的所有同步原语的等价物。例如，可以使用锁来确保一次只有一个进程打印到标准输出:

```py
from multiprocessing import Process, Lock

def f(l, i):
    l.acquire()
    try:
        print('hello world', i)
    finally:
        l.release()

if __name__ == '__main__':
    lock = Lock()

    for num in range(10):
        Process(target=f, args=(lock, num)).start()
```

不使用锁的情况下，来自于多进程的输出很容易产生混淆。

## 进程间共享状态

进程之间共享数据，都需要使用某个变量。

multiprocessing 提供了**共享内存**和**服务进程**两种方法。

### 共享内存

可以使用 `Value` 或 `Array` 将数据存储在共享内存映射中。例如，以下代码:

```py
from multiprocessing import Process, Value, Array

def f(n, a):
    n.value = 3.1415927
    for i in range(len(a)):
        a[i] = -a[i]

if __name__ == '__main__':
    num = Value('d', 0.0)
    arr = Array('i', range(10))

    p = Process(target=f, args=(num, arr))
    p.start()
    p.join()

    print(num.value)
    print(arr[:])
```

将打印

```txt
3.1415927
[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
```

创建 `num` 和 `arr` 时使用的 'd' 和 'i' 参数是 array 模块使用的类型的 typecode ： 'd' 表示双精度浮点数， 'i' 表示有符号整数。这些共享对象将是进程和线程安全的。

### 服务进程（Manager）

由 `Manager()` 返回的管理器对象控制一个服务进程，该进程保存 `Python` 对象并允许其他进程使用代理操作它们。

`Manager()` 返回的管理器支持类型： `list 、 dict 、 Namespace 、 Lock 、 RLock 、 Semaphore 、 BoundedSemaphore 、 Condition 、 Event 、 Barrier 、 Queue 、 Value 和 Array` 。例如

```py
from multiprocessing import Process, Manager

def f(d, l):
    d[1] = '1'
    d['2'] = 2
    d[0.25] = None
    l.reverse()

if __name__ == '__main__':
    with Manager() as manager:
        d = manager.dict()
        l = manager.list(range(10))

        p = Process(target=f, args=(d, l))
        p.start()
        p.join()

        print(d)
        print(l)
```

将打印

```txt
{0.25: None, 1: '1', '2': 2}
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
```

使用服务进程的管理器比使用共享内存对象更灵活，因为它们可以支持任意对象类型。

## multiprocessing.Lock() 与 Manager().Lock() 的区别

区别：

- 普通 `multiprocessing.Lock()`
  - 由父进程创建，并直接传递给子进程。
    - 作用域限制：只能在同一父进程创建的多个子进程之间共享，无法跨不同父进程或独立进程使用。
    - 实现机制：基于操作系统原生的同步原语（如POSIX互斥锁），直接在进程间共享内存。
- `Manager().Lock()`
  - 通过 `multiprocessing.Manager()` 服务进程创建，是一个代理对象。
    - 跨进程共享：可被任意连接到同一 `Manager` 的进程共享，即使这些进程无父子关系。
    - 实现机制：锁状态由 `Manager` 服务进程集中管理，所有操作通过进程间通信（IPC）完成。

适用场景：

普通Lock适用场景  
多个子进程由同一父进程派生，且需要同步访问共享资源（如内存中的数据结构）。

```python
from multiprocessing import Process, Lock

def worker(lock):
    with lock:
        # 操作共享资源
        pass

if __name__ == '__main__':
    lock = Lock()
    processes = [Process(target=worker, args=(lock,)) for _ in range(4)]
    for p in processes:
        p.start()
    for p in processes:
        p.join()
```

`Manager` 的 `Lock` 适用场景  
跨独立进程或需要动态连接（如网络分布式环境）的同步需求。

```python
from multiprocessing import Process, Manager

def worker(lock):
    with lock:
        # 操作共享资源
        pass

if __name__ == '__main__':
    with Manager() as manager:
        lock = manager.Lock()
        processes = [Process(target=worker, args=(lock,)) for _ in range(4)]
        for p in processes:
            p.start()
        for p in processes:
            p.join()
```

## 参考

### Process 类

**构造函数**：

`class multiprocessing.Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)`

**属性、方法**：

- `join([timeout])`
  - 作用
    - 如果可选参数 `timeout` 是 `None` （默认值），则该方法将阻塞，直到调用 `join()` 方法的进程终止。
    - 如果 `timeout` 是一个正数，它最多会阻塞 `timeout` 秒。请注意，如果进程终止或方法超时，则该方法返回 None 。检查进程的 exitcode 以确定它是否终止。
  - 一个进程可以被 `join` 多次。
  - 进程无法 `join` 自身，因为这会导致死锁。尝试在启动进程之前 `join` 进程是错误的。
- `name`
  - 进程的名称。该名称是一个字符串，仅用于识别目的。它没有语义。可以为多个进程指定相同的名称。
  - 初始名称由构造器设定。 如果没有为构造器提供显式名称，则会构造一个形式为 'Process-N1:N2:...:Nk' 的名称，其中每个 Nk 是其父亲的第 N 个孩子。
- `is_alive()`
  - 返回进程是否还活着。
  - 粗略地说，从 `start()` 方法返回到子进程终止之前，进程对象仍处于活动状态。
- `terminate()`
  - 终结进程。
