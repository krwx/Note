# 进程和线程

- [进程和线程](#进程和线程)
  - [多进程](#多进程)
    - [启动方法](#启动方法)
    - [Process](#process)
    - [Pool](#pool)
      - [参考](#参考)
      - [使用](#使用)
      - [AsyncResult](#asyncresult)
    - [进程间通信](#进程间通信)
      - [Queue](#queue)
      - [Pipe](#pipe)
    - [进程间同步](#进程间同步)
    - [进程间共享状态](#进程间共享状态)
      - [共享内存](#共享内存)
      - [服务进程（Manager）](#服务进程manager)
    - [multiprocessing.Lock() 与 Manager().Lock() 的区别](#multiprocessinglock-与-managerlock-的区别)
    - [参考](#参考-1)
      - [Process 类](#process-类)
  - [多线程](#多线程)
    - [线程模块](#线程模块)
    - [使用线程](#使用线程)
    - [锁](#锁)
    - [多线程使用单核](#多线程使用单核)

对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。

有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。

## 多进程

### 启动方法

multiprocessing 支持三种启动进程的方法。这些 启动方法 有

- `spawn`
  - 父进程会启动一个新的 Python 解释器进程。 子进程将只继承那些运行进程对象的 run() 方法所必须的资源。 特别地，来自父进程的非必需文件描述符和句柄将不会被继承。 使用此方法启动进程相比使用 fork 或 forkserver 要慢上许多。
  - 在 POSIX 和 `Windows` 平台上可用。 默认在 Windows 和 macOS 上。
- `fork`
  - 父进程使用 `os.fork()` 来产生 Python 解释器分叉。子进程在开始时实际上与父进程相同。父进程的所有资源都由子进程继承。请注意，安全分叉多线程进程是棘手的。
  - 主要在 Unix/Linux/Mac 系统使用

有多种启动进程方法的原因是适配不同的平台

### Process

使用 Process 类创建一个进程：

```py
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
```

执行结果：

```txt
Parent process 928.
Child process will start.
Run child process test (929)...
Process end.
```

创建子进程时，**只需要传入一个执行函数和函数的参数（注意为 `args=('test',)` 这种格式）**，创建一个 `Process` 实例，用 `start()` 方法启动，用 `join()` 方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。

### Pool

#### 参考

语法：`Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])¶`

- `processes` 是要使用的工作进程数目。如果 `processes` 为 `None`，则使用 `os.cpu_count()` 返回的值。(3.13 使用 `os.process_cpu_count()`)

`Pool` 的方法：

- `apply(func[, args[, kwds]])`
  - **同步执行，会阻塞**
  - 使用 `args` 参数以及 `kwds` 命名参数调用 `func` , 它会**返回结果前阻塞**。
- `apply_async(func[, args[, kwds[, callback[, error_callback]]]])`
  - **异步执行，不会阻塞**，更适合并行化工作
  - `apply()` 方法的一个变种，返回一个 `AsyncResult` 对象。
  - 如果指定了 `callback` , 它必须是一个**接受单个参数的可调用对象**。
  - 当执行成功时，`callback` 会被用于处理执行后的返回结果，否则，调用 `error_callback` 。
  - 回调函数应该立即执行完成，否则会阻塞负责处理结果的线程。
- `close()`
  - 阻止后续任务提交到进程池，当所有任务执行完成后，工作进程会退出。
- `terminate()`
  - 不必等待未完成的任务，立即停止工作进程。当进程池对象被垃圾回收时，会立即调用 `terminate()`。
- `join()`
  - 等待工作进程结束。调用 `join()` 前必须先调用 `close()` 或者 `terminate()` 。

#### 使用

如果要启动大量的子进程，可以用进程池的方式批量创建子进程：

```py
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Pool(4) # 最多跑 4 个进程
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
```

执行结果如下：

```txt
Parent process 669.
Waiting for all subprocesses done...
Run task 0 (671)...
Run task 1 (672)...
Run task 2 (673)...
Run task 3 (674)...
Task 2 runs 0.14 seconds.
Run task 4 (673)...
Task 1 runs 0.27 seconds.
Task 3 runs 0.86 seconds.
Task 0 runs 1.41 seconds.
Task 4 runs 1.91 seconds.
All subprocesses done.
```

对 `Pool` 对象调用 `join()` 方法会**等待所有子进程执行完毕**，调用 `join()` 之前必须先调用 `close()` ，调用 `close()` 之后就不能继续添加新的 `Process`了。

请注意输出的结果，`task 0，1，2，3` 是立刻执行的，而 `task 4` 要等待前面某个 `task` 完成后才执行

#### AsyncResult

`Pool.apply_async()` 和 `Pool.map_async()` 返回对象所属的类。

- `get([timeout])`
  - 用于获取执行结果。
  - 如果 `timeout` 不是 `None` 并且在 `timeout` 秒内仍然没有执行完得到结果，则抛出 `multiprocessing.TimeoutError` 异常。
- `wait([timeout])`
  - 阻塞，直到返回结果，或者 `timeout` 秒后超时。
- `ready()`
  - 返回执行状态，是否已经完成。
- `successful()`
  - 判断调用是否已经完成并且未引发异常。 如果还未获得结果则将引发 `ValueError`。

在 `pool.join()` 前调用 `get()`，会造成阻塞：

```py
from multiprocessing import Pool
import time, random


def f(x):
    time.sleep(random.random() * 3)
    return x * x


def main():
    pool = Pool(processes=4)
    t_start = time.time()
    for i in range(5):
        result = pool.apply_async(f, (i,))
        print(result.get())
    pool.close()
    pool.join()
    print(time.time() - t_start)

""" 
输出：
0
1
4
9
16
7.997464895248413
"""
```

在 `pool.join()` 后调用 `get()`，不会造成阻塞：

```py
def f(x):
    time.sleep(random.random() * 3)
    return x * x


def main():
    pool = Pool(processes=4)
    t_start = time.time()
    res_list = []
    for i in range(5):
        result = pool.apply_async(f, (i,))
        res_list.append(result)
    pool.close()
    pool.join()
    for res in res_list:
        print(res.get())
    print(time.time() - t_start)

""" 
输出：
9
0
16
1
4
2.55433548671
"""
```

### 进程间通信

Python 的 `multiprocessing` 模块包装了底层的机制，提供了 `Queue、Pipes` 多种方式来交换数据。

进程之间发送数据

#### Queue

我们以 `Queue` 为例，在父进程中创建两个子进程，一个往 `Queue` 里写数据，一个从 `Queue` 里读数据：

```py
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
```

运行结果如下：

```txt
Process to write: 50563
Put A to queue...
Process to read: 50564
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.
```

#### Pipe

`Pipe()` 函数返回一个由管道连接的连接对象，默认情况下是双工（双向）。例如:

```py
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # 打印 "[42, None, 'hello']"
    p.join()
```

返回的两个连接对象 `Pipe()` 表示管道的两端。每个连接对象都有 `send()` 和 `recv()` 方法（相互之间的）。请注意，如果两个进程（或线程）同时尝试读取或写入管道的 同一 端，则管道中的数据可能会损坏。

### 进程间同步

`multiprocessing` 包含来自 `threading` 的所有同步原语的等价物。例如，可以使用锁来确保一次只有一个进程打印到标准输出:

```py
from multiprocessing import Process, Lock

def f(l, i):
    l.acquire()
    try:
        print('hello world', i)
    finally:
        l.release()

if __name__ == '__main__':
    lock = Lock()

    for num in range(10):
        Process(target=f, args=(lock, num)).start()
```

不使用锁的情况下，来自于多进程的输出很容易产生混淆。

### 进程间共享状态

进程之间共享数据，都需要使用某个变量。

multiprocessing 提供了**共享内存**和**服务进程**两种方法。

#### 共享内存

可以使用 `Value` 或 `Array` 将数据存储在共享内存映射中。例如，以下代码:

```py
from multiprocessing import Process, Value, Array

def f(n, a):
    n.value = 3.1415927
    for i in range(len(a)):
        a[i] = -a[i]

if __name__ == '__main__':
    num = Value('d', 0.0)
    arr = Array('i', range(10))

    p = Process(target=f, args=(num, arr))
    p.start()
    p.join()

    print(num.value)
    print(arr[:])
```

将打印

```txt
3.1415927
[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
```

创建 `num` 和 `arr` 时使用的 'd' 和 'i' 参数是 array 模块使用的类型的 typecode ： 'd' 表示双精度浮点数， 'i' 表示有符号整数。这些共享对象将是进程和线程安全的。

#### 服务进程（Manager）

由 `Manager()` 返回的管理器对象控制一个服务进程，该进程保存 `Python` 对象并允许其他进程使用代理操作它们。

`Manager()` 返回的管理器支持类型： `list 、 dict 、 Namespace 、 Lock 、 RLock 、 Semaphore 、 BoundedSemaphore 、 Condition 、 Event 、 Barrier 、 Queue 、 Value 和 Array` 。例如

```py
from multiprocessing import Process, Manager

def f(d, l):
    d[1] = '1'
    d['2'] = 2
    d[0.25] = None
    l.reverse()

if __name__ == '__main__':
    with Manager() as manager:
        d = manager.dict()
        l = manager.list(range(10))

        p = Process(target=f, args=(d, l))
        p.start()
        p.join()

        print(d)
        print(l)
```

将打印

```txt
{0.25: None, 1: '1', '2': 2}
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
```

使用服务进程的管理器比使用共享内存对象更灵活，因为它们可以支持任意对象类型。

### multiprocessing.Lock() 与 Manager().Lock() 的区别

区别：

- 普通 `multiprocessing.Lock()`
  - 由父进程创建，并直接传递给子进程。
    - 作用域限制：只能在同一父进程创建的多个子进程之间共享，无法跨不同父进程或独立进程使用。
    - 实现机制：基于操作系统原生的同步原语（如POSIX互斥锁），直接在进程间共享内存。
- `Manager().Lock()`
  - 通过 `multiprocessing.Manager()` 服务进程创建，是一个代理对象。
    - 跨进程共享：可被任意连接到同一 `Manager` 的进程共享，即使这些进程无父子关系。
    - 实现机制：锁状态由 `Manager` 服务进程集中管理，所有操作通过进程间通信（IPC）完成。

适用场景：

普通Lock适用场景  
多个子进程由同一父进程派生，且需要同步访问共享资源（如内存中的数据结构）。

```python
from multiprocessing import Process, Lock

def worker(lock):
    with lock:
        # 操作共享资源
        pass

if __name__ == '__main__':
    lock = Lock()
    processes = [Process(target=worker, args=(lock,)) for _ in range(4)]
    for p in processes:
        p.start()
    for p in processes:
        p.join()
```

`Manager` 的 `Lock` 适用场景  
跨独立进程或需要动态连接（如网络分布式环境）的同步需求。

```python
from multiprocessing import Process, Manager

def worker(lock):
    with lock:
        # 操作共享资源
        pass

if __name__ == '__main__':
    with Manager() as manager:
        lock = manager.Lock()
        processes = [Process(target=worker, args=(lock,)) for _ in range(4)]
        for p in processes:
            p.start()
        for p in processes:
            p.join()
```

### 参考

#### Process 类

- `join([timeout])`
  - 作用
    - 如果可选参数 `timeout` 是 `None` （默认值），则该方法将阻塞，直到调用 `join()` 方法的进程终止。
    - 如果 `timeout` 是一个正数，它最多会阻塞 `timeout` 秒。请注意，如果进程终止或方法超时，则该方法返回 None 。检查进程的 exitcode 以确定它是否终止。
  - 一个进程可以被 `join` 多次。
  - 进程无法 `join` 自身，因为这会导致死锁。尝试在启动进程之前 `join` 进程是错误的。
- `name`
  - 进程的名称。该名称是一个字符串，仅用于识别目的。它没有语义。可以为多个进程指定相同的名称。
  - 初始名称由构造器设定。 如果没有为构造器提供显式名称，则会构造一个形式为 'Process-N1:N2:...:Nk' 的名称，其中每个 Nk 是其父亲的第 N 个孩子。
- `is_alive()`
  - 返回进程是否还活着。
  - 粗略地说，从 `start()` 方法返回到子进程终止之前，进程对象仍处于活动状态。
- `terminate()`
  - 终结进程。

## 多线程

使用 `threading` 模块实现多线程

### 线程模块

threading 模块提供的方法：

- `threading.current_thread()`: 返回当前的线程变量。
- `threading.enumerate()`: 返回一个包含正在运行的线程的列表。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。
- `threading.active_count()`: 返回正在运行的线程数量，与 `len(threading.enumerate())` 有相同的结果。
- `threading.Thread(target, args=(), kwargs={}, daemon=None)`：创建 `Thread` 类的实例。
  - `target`：线程将要执行的目标函数。
  - `args`：目标函数的参数，以元组形式传递。
  - `kwargs`：目标函数的关键字参数，以字典形式传递。
  - `daemon`：指定线程是否为守护线程。

Thread 类的方法：

- `start()`：启动线程
- `run()`：线程在此方法中定义要执行的代码
- `join(self, timeout=None)`：等待线程终止
  - 默认情况下，`join()` 会一直阻塞，直到被调用线程终止。如果指定了 `timeout` 参数，则最多等待 `timeout` 秒
- `is_alive()`：返回线程是否在运行
  - 如果线程已经启动且尚未终止，则返回 True，否则返回 False。
- `getName()`：返回线程的名称
- `setName()`：设置线程的名称
- `ident` 属性：线程的唯一标识符
- `daemon` 属性：线程的守护标志，用于指示是否是守护线程

### 使用线程

启动一个线程就是把一个函数传入并创建 `Thread` 实例，然后调用 `start()` 开始执行:

```py
import time, threading

def loop():
    print("thread %s is running" % threading.current_thread().name)
    n = 0
    while n < 5:
        n = n + 1
        print('thread %s >>> %s' % (threading.current_thread().name, n))
        time.sleep(1)
    print("thread %s ended." % threading.current_thread().name)

def main():
    print("thread %s is running" % threading.current_thread().name)
    t = threading.Thread(target=loop, name='LoopThread')
    t.start()
    t.join()
    print("thread %s ended." % threading.current_thread().name)


if __name__ == "__main__":
    main()
```

执行结果如下：

```txt
thread MainThread is running...
thread LoopThread is running...
thread LoopThread >>> 1
thread LoopThread >>> 2
thread LoopThread >>> 3
thread LoopThread >>> 4
thread LoopThread >>> 5
thread LoopThread ended.
thread MainThread ended.
```

### 锁

使用 `Thread` 对象的 `Lock` 和 `Rlock` 可以实现简单的线程同步，这两个对象都有 `acquire` 方法和 `release` 方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到 acquire 和 release 方法之间。

```py
#!/usr/bin/python3

import threading
import time

class myThread (threading.Thread):
    def __init__(self, threadID, name, delay):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.delay = delay
    def run(self):
        print ("开启线程： " + self.name)
        # 获取锁，用于线程同步
        threadLock.acquire()
        print_time(self.name, self.delay, 3)
        # 释放锁，开启下一个线程
        threadLock.release()

def print_time(threadName, delay, counter):
    while counter:
        time.sleep(delay)
        print ("%s: %s" % (threadName, time.ctime(time.time())))
        counter -= 1

threadLock = threading.Lock()
threads = []

# 创建新线程
thread1 = myThread(1, "Thread-1", 1)
thread2 = myThread(2, "Thread-2", 2)

# 开启新线程
thread1.start()
thread2.start()

# 添加线程到线程列表
threads.append(thread1)
threads.append(thread2)

# 等待所有线程完成
for t in threads:
    t.join()
print ("退出主线程")
```

在 `with` 语句中使用锁:

```py
with some_lock:
    # 执行某种操作...

# 相当于
some_lock.acquire()
try:
    # 执行某种操作...
finally:
    some_lock.release()
```

### 多线程使用单核

`Python` 的线程虽然是真正的线程，但解释器执行代码时，有一个 `GIL` 锁：`Global Interpreter Lock`，任何 `Python` 线程执行前，必须先获得 `GIL` 锁，然后，每执行 100 条字节码，解释器就自动释放 `GIL` 锁，让别的线程有机会执行。这个 `GIL` 全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在 `Python` 中只能交替执行，即使 100 个线程跑在 100 核CPU上，也只能用到 1 个核。

但是用 C、C++ 或 Java 来写多线程，可以把全部核心跑满
