# 简单使用

前提：安装好 celery，选择 RabbitMQ 作为 broker 和 backend

- [简单使用](#简单使用)
  - [创建 celery 实例和声明 task](#创建-celery-实例和声明-task)
  - [项目结构](#项目结构)
    - [教程的结构](#教程的结构)
    - [实际项目的结构](#实际项目的结构)
  - [配置](#配置)
    - [result\_backend 使用 SQLAlchemy](#result_backend-使用-sqlalchemy)
  - [启动 Celery](#启动-celery)
    - [--app 参数](#--app-参数)
    - [multi](#multi)
  - [task](#task)
    - [调用 task](#调用-task)
    - [任务的状态](#任务的状态)
    - [signature](#signature)
      - [使用 signature](#使用-signature)
      - [使用 Primitives](#使用-primitives)
        - [group](#group)
        - [chain](#chain)
  - [Routing](#routing)

## 创建 celery 实例和声明 task

```py
# task.py
from celery import Celery

app = Celery('tasks', broker='amqp://', backend='rpc://',)

@app.task
def add(x, y):
    return x + y
```

- Celery()
  - main
    - 第一个参数是当前模块的名称。
    - 这是必需的，为了在 `__main__` 模块中定义任务时自动生成名称。
  - broker
    - 第二个参数是 broker 关键字参数。
    - 这里我们使用的是 RabbitMQ（也是默认选项）。
  - backend
    - 指定 task 结果存储的位置
    - 如果 task 是 ignore result 的，那么结果不会存储

## 项目结构

### 教程的结构

简单的结构：

```txt
src/
    proj/__init__.py
        /celery.py
        /tasks.py
```

`task` 在另外的 `py` 文件声明，需要在创建 `Celery` 实例时通过 `include` 将 `task` 所处的模块引入到 `Celery` 实例

```py
# celery.py
from celery import Celery

app = Celery('proj',
             broker='amqp://',
             backend='rpc://',
             include=['proj.tasks']) # 引入 task 模块

# Optional configuration, see the application user guide.
app.conf.update(
    result_expires=3600,
)

if __name__ == '__main__':
    app.start()
```

声明 task

```py
# task.py
from .celery import app


@app.task
def add(x, y):
    return x + y
```

这个结构运行的时候实际会报错：`ImportError: cannot import name 'Celery' from partially initialized module 'celery'，`

原因：项目中的 `celery.py` 文件与 `Python` 的 `celery` 库同名，导致导入时优先引用当前目录下的文件而非第三方库。

### 实际项目的结构

1、将 `celery.py` 放在子目录中，避免与顶级模块名冲突：

```txt
myproject/
├── __init__.py
├── celery_app/    # 新建子目录
│   ├── __init__.py
│   └── celery.py  # 原 celery.py 文件
└── tasks.py
```

2、在 `myproject/__init__.py` 中添加：

```python
# myproject/__init__.py
from .celery_app.celery import app as celery_app

__all__ = ('celery_app',)
```

3、在 `tasks.py` 中通过项目模块导入：

```python
# tasks.py
from myproject import celery_app

@celery_app.task
def my_task():
    return "Task executed!"
```

4、运行 `Worker` 时指定完整模块路径：

```bash
celery -A myproject.celery_app.celery worker --loglevel=info
```

## 配置

配置单项：

```py
app.conf.task_serializer = 'json'
```

配置多项：

```py
app.conf.update(
    task_serializer='json',
    accept_content=['json'],  # Ignore other content
    result_serializer='json',
    timezone='Europe/Oslo',
    enable_utc=True,
)
```

大型项目推荐使用配置模块配置，调用 `app.config_from_object()` 方法使用模块进行配置

```py
app.config_from_object('celeryconfig')
# celeryconfig 是模块名
```

`celeryconfig.py` 的例子：

```py
broker_url = 'pyamqp://'
result_backend = 'rpc://'

task_serializer = 'json'
result_serializer = 'json'
accept_content = ['json']
timezone = 'Europe/Oslo'
enable_utc = True

task_annotations = {
    'tasks.add': {'rate_limit': '10/m'} # 限制该 task 一分钟最多只能运行 10 个
}
```

### result_backend 使用 SQLAlchemy

- database 的 url 怎么填
- 启动后要保存结果时才创建表，但是 5.5 版本加了个设置可以启动后就创建表
  - 创建了两个表，一个是 task 用，一个是 group 用
  - 默认的表名为 celery_taskmeta 和 celery_tasksetmeta
  - 可以设置表名和 schema 名
  - 要设置 result_extend 才能存放参数那些数据
- 启动之后发现插入数据报错了
  - 原来是 sequence 的数据类型是 bigint，但是表的 id 的数据类型是 int。改数据类型就好了。
- task id 是 unique，要删除的话需要删除 unique key
- 如果调用 task 是 ignore result 的话，那么不会存放结果
  - 不管结果的话，不操作返回的 Async Result 就不会产生同步操作。
  - 直接 apply_async 或者 delay 就好了

## 启动 Celery

运行命令启动 worker ：`celery -A tasks worker --loglevel=INFO`

> 在生产环境中，您希望在后台将 `worker` 作为守护进程运行。为此，您需要使用平台提供的工具，或类似 `supervisord`的工具

### --app 参数

`--app = -A`

`--app` 参数后面指定的值以 `module.path:attribute` 的格式指定要使用的 `Celery` 应用程序实例

但是简短点可以直接指定 `module.path`，那么它会在该路径下寻找实例，规则如下，以 `--app=proj` 为例 ：

- 在 `proj` 模块下找
  - 名字为 `app` 的属性
  - 名字为 `celery` 的属性
  - 值为 `Celery application` 的属性
- 如果上面的找不到，那么在 `proj.celery` 模块下找
  - 名字为 `app` 的属性
  - 名字为 `celery` 的属性
  - 值为 `Celery application` 的属性

### multi

守护进程脚本使用 `celery multi` 命令在后台启动一个或多个工作进程：

```sh
celery multi start w1 -A proj -l INFO

# restart
celery  multi restart w1 -A proj -l INFO

# stop immediately
celery multi stop w1 -A proj -l INFO

# stop but wait for all tasks are completed
celery multi stopwait w1 -A proj -l INFO
```

## task

### 调用 task

使用 `delay()` 或 `apply_async()` 方法调用 task

调用 task 将返回一个 `AsyncResult` 实例。 这可以用来检查任务的状态，或等待任务完成，或获取其返回值（或者如果任务失败，则获取异常和回溯）。

```py
from tasks import add
result = add.delay(4, 4)
```

`ready()` 方法返回任务是否已完成处理

```py
result.ready() # False or True
```

您可以等待结果完成，但这很少使用 因为它将异步调用转换为同步调用

```py
result.get(timeout=1) # 8
```

如果任务引发异常，`get()` 将重新抛出异常，但您可以通过指定 propagate 参数覆盖该行为：

```py
result.get(propagate=False)
```

如果任务引发了异常，您还可以访问原始的 traceback：

```py
result.traceback
```

> 注意：为了确保资源被释放，您最终必须对调用任务后返回的每个 `AsyncResult` 实例调用 `get()` 或 `forget()` 。

### 任务的状态

状态：

- FAILURE
- PENDING
- RECEIVED
- RETRY
- REVOKED
- STARTED
- SUCCESS

通常的任务的状态走向：`PENDING -> STARTED -> SUCCESS`

> `STARTED` 状态是一种特殊状态，只有在启用了 `task_track_started` 设置，或者为任务设置了 `@task(track_started=True)` 选项时才会记录。

重试两次的任务的状态走向：`PENDING -> STARTED -> RETRY -> STARTED -> RETRY -> STARTED -> SUCCESS`

### signature

signature 将单个任务调用的参数和执行选项包装在一起，这样它就可以传递给函数，甚至可以序列化并通过网络发送。

例子，调用 add task，并传递 (2,2) 参数和指定 countdown 等于 10：

```py
add.signature((2, 2), countdown=10)

# 缩写
add.s(2, 2)
```

#### 使用 signature

使用 `delay()` 或 `apply_async()` 调用 `signature`

1、signature 指定了所有参数

```py
s1 = add.s(2, 2) # 声明签名
res = s1.delay() # 调用签名
res.get() # 4
```

2、signature 指定了部分参数

```py
# incomplete partial: add(?, 2)
s2 = add.s(2)

# resolves the partial: add(8, 2)
res = s2.delay(8) 
res.get() # result: 10
```

3、signature 指定 Keyword arguments

如果调用时也指定了相同的 Keyword arguments，那么以调用时的参数为准

```py
s3 = add.s(2, 2, debug=True)
s3.delay(debug=False)   # debug is now False.
```

#### 使用 Primitives

- group
- chain
- chord
  - rpc 用不了，即 RabbitMQ 作为 backend 用不了
- map
- starmap
- chunks

`primitives` 本身就是 `signature`，因此可以组合它们以多种方式编写复杂的工作流。

##### group

`group()` 并行调用一堆 task，并返回一个结果组，按顺序获取 task 的结果

```py
from celery import group
from proj.tasks import add

group(add.s(i, i) for i in range(10))().get()
# [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
```

签名指定部分参数

```py
g = group(add.s(i) for i in range(10))
g(10).get()
# [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
```

##### chain

使用 `chain()` 可以让任务链接在一起，这样在一个任务返回后，另一个任务就会被调用

```py
from celery import chain
from proj.tasks import add, mul

# (4 + 4) * 8
chain(add.s(4, 4) | mul.s(8))().get()
# 64
```

签名指定部分参数

```py
# (? + 4) * 8
g = chain(add.s(4) | mul.s(8))
g(4).get()
64
```

缩写，直接不写 chain

```py
(add.s(4, 4) | mul.s(8))().get()
# 64
```

## Routing

celery 支持将消息发送到命名队列的简单路由

`app.conf` 的 `task_routes` 设置允许您能够按名称路由任务：

```py
app.conf.update(
    task_routes = {
        'proj.tasks.add': {'queue': 'hipri'}, # 指定发送的任务队列
    },
)
```

在调用 `apply_async()` 也可以指定队列

```py
from proj.tasks import add
add.apply_async((2, 2), queue='hipri')
```

然后，您可以通过指定 `celery worker` 的 `-Q` 选项使 `worker` 从该队列中消费：

```sh
celery -A proj worker -Q hipri

# 指定消费多个队列，用 "," 隔开
celery -A proj worker -Q hipri,celery
```

队列的顺序并不重要，因为 worker 会给队列同等的权重。

> 由于历史原因，默认的队列的名称为 `celery`
